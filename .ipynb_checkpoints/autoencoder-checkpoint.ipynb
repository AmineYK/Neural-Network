{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf42b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from modules import *\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from os import listdir\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import rand_score,adjusted_rand_score\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9568321",
   "metadata": {},
   "source": [
    "# Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f90651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(datas,labels,verbose=False):\n",
    "    titles = [\"X\",\"Representation Latente\",\"X_hat\"]\n",
    "    scores = []\n",
    "    puritities = []\n",
    "    rand_scores= []\n",
    "    for i,data in enumerate(datas):\n",
    "\n",
    "        shape = data.shape[1]\n",
    "        nb_classes = len(np.unique(labels))\n",
    "\n",
    "        neuro_i_1 = shape\n",
    "        neuro_o_1 = shape // 2\n",
    "        neuro_i_2 = shape // 2\n",
    "        neuro_o_2 = shape // 4\n",
    "        neuro_i_3 = shape // 4\n",
    "        neuro_o_3 = nb_classes\n",
    "        facteur_norma = 0.4\n",
    "        plage_biais = (0,1)\n",
    "        batch_size = 100\n",
    "        epochs = 50\n",
    "\n",
    "\n",
    "        facteur_norma = 0.8\n",
    "        lineaire_1 = ModuleLineaire(neuro_i_1 ,neuro_o_1 ,plage_biais,facteur_norma)\n",
    "        lineaire_2 = ModuleLineaire(neuro_i_2 ,neuro_o_2 ,plage_biais,facteur_norma)\n",
    "        lineaire_3 = ModuleLineaire(neuro_i_3 ,neuro_o_3 ,plage_biais,facteur_norma)\n",
    "        TanH = ModuleTanH()\n",
    "        sigmoide = ModuleSigmoide()\n",
    "        mseloss = MSELoss()\n",
    "        CE = CrossEntropieLoss(nb_classes)\n",
    "        softmax = SoftMax()\n",
    "\n",
    "\n",
    "        network_layers = [lineaire_1,TanH,lineaire_2,TanH,lineaire_3,softmax]\n",
    "        network = Sequentiel(network_layers)\n",
    "\n",
    "        if verbose : print(\"Optimisation de : \",titles[i])\n",
    "        opti = Optim(network,CE,1e-2)\n",
    "        opti.SGD(data,labels,batch_size,epochs)\n",
    "\n",
    "        score = opti.getNetwork().accuracy(data,labels) \n",
    "        scores.append(score)\n",
    "        if verbose : print(\"Accuracy sur les images issues de l'auto encodeur\",score)    \n",
    "        \n",
    "        kmeans = KMeans(n_clusters=10, random_state=0, max_iter=1000).fit(data)\n",
    "        yhat = kmeans.labels_\n",
    "        \n",
    "        \n",
    "        y_cluster_pred = np.zeros(len(data))\n",
    "        indices = np.arange(len(data))\n",
    "\n",
    "        for cluster in range(3):\n",
    "\n",
    "            vals, counts = np.unique(labels[yhat == cluster], return_counts=True)\n",
    "            val_maj = vals[np.argmax(counts)]\n",
    "\n",
    "            y_cluster_pred[indices[yhat == cluster]] = val_maj\n",
    "        \n",
    "        purete = cluster_purity(labels,y_cluster_pred)\n",
    "        puritities.append(purete)\n",
    "        \n",
    "        rs  = rand_score(labels,y_cluster_pred)\n",
    "        rand_scores.append(rs)\n",
    "        \n",
    "        if verbose : \n",
    "            print(\"Puret√© du clustering : \",purete)\n",
    "            print(\"Rand score  : \",rs)\n",
    "            print(\"Adjusted Rand Score : \",adjusted_rand_score(labels,y_cluster_pred))\n",
    "            print()\n",
    "\n",
    "        \n",
    "        \n",
    "    return scores,puritities,rand_scores\n",
    "        \n",
    "        \n",
    "        \n",
    "def optimisation_espace_latent(data,labels,dim_espace_latent_list,eps,verbose=False):\n",
    "    \n",
    "    nb_classes = len(np.unique(labels))\n",
    "    n_samples = data.shape[0]\n",
    "    scores = []\n",
    "    puritities = []\n",
    "    rand_scores= []\n",
    "    \n",
    "    \n",
    "    for dim_espace_latent in dim_espace_latent_list:\n",
    "        \n",
    "\n",
    "        neuro_i_1 = 256\n",
    "        neuro_o_1 = 160\n",
    "        neuro_i_2 = 160\n",
    "        neuro_o_2 = 120\n",
    "        neuro_i_3 = 120\n",
    "        neuro_o_3 = 60\n",
    "        neuro_i_4 = 60\n",
    "        neuro_o_4 = dim_espace_latent\n",
    "        \n",
    "        facteur_norma = 0.4\n",
    "        plage_biais = (0,1)\n",
    "        batch_size = 100\n",
    "        epochs = 100\n",
    "        nb_couches = 4\n",
    "\n",
    "\n",
    "        facteur_norma = 0.8\n",
    "\n",
    "\n",
    "        lineaire_1_enc = ModuleLineaire(neuro_i_1 ,neuro_o_1 ,plage_biais,facteur_norma)\n",
    "        lineaire_2_enc = ModuleLineaire(neuro_i_2 ,neuro_o_2 ,plage_biais,facteur_norma)\n",
    "        lineaire_3_enc = ModuleLineaire(neuro_i_3 ,neuro_o_3 ,plage_biais,facteur_norma)\n",
    "        lineaire_4_enc = ModuleLineaire(neuro_i_4 ,neuro_o_4 ,plage_biais,facteur_norma)\n",
    "\n",
    "        lineaire_1_dec = ModuleLineaire(neuro_o_1 ,neuro_i_1 ,plage_biais,facteur_norma)\n",
    "        lineaire_2_dec = ModuleLineaire(neuro_o_2 ,neuro_i_2 ,plage_biais,facteur_norma)\n",
    "        lineaire_3_dec = ModuleLineaire(neuro_o_3 ,neuro_i_3 ,plage_biais,facteur_norma)\n",
    "        lineaire_4_dec = ModuleLineaire(neuro_o_4 ,neuro_i_4 ,plage_biais,facteur_norma)\n",
    "\n",
    "        TanH = ModuleTanH()\n",
    "        sigmoide = ModuleSigmoide()\n",
    "        BCE = BinaryCrossEntropie()\n",
    "\n",
    "\n",
    "        network_layers = [lineaire_1_enc,TanH,lineaire_2_enc,TanH,lineaire_3_enc,TanH,lineaire_4_enc,TanH,\n",
    "                          lineaire_4_dec,TanH,lineaire_3_dec,TanH,lineaire_2_dec,TanH,lineaire_1_dec,sigmoide]\n",
    "        network = Sequentiel(network_layers)\n",
    "\n",
    "        auto_encodeur_usps = AutoEncodeur(network,BCE)\n",
    "\n",
    "        \n",
    "        if verbose : print(\"Dimension espace latent : \",dim_espace_latent)\n",
    "        \n",
    "        auto_encodeur_usps.optimisation(data,labels,batch_size,epochs,eps,False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        latent_repre = auto_encodeur_usps.encode(data)\n",
    "        \n",
    "        datas = [latent_repre]\n",
    "        score,purete,rand_score = evaluation(datas,labels,verbose)\n",
    "        scores.append(score)\n",
    "        puritities.append(purete)\n",
    "        rand_scores.append(rand_score)\n",
    "        \n",
    "    return scores , puritities , rand_scores\n",
    "\n",
    "\n",
    "\n",
    "def create_auto_encodeur(X_train,output_dim, nb_couche_lin,activations, facteur_norma, plage_biais, batch_size, epochs,eps=1e-4):\n",
    "    np.random.seed(42)\n",
    "    input_dim = X_train.shape[1]\n",
    "    n_samples = X_train.shape[0]\n",
    "    neuro_in = [input_dim]\n",
    "    neuro_out = []\n",
    "    activations_enc = activations[0]\n",
    "    activations_dec = activations[1]\n",
    "    assert len(activations_enc) == nb_couche_lin and len(activations_dec) == nb_couche_lin\n",
    "    for i in range(0,nb_couche_lin) : \n",
    "        neuro_out.append(neuro_in[::-1][0] // 2)\n",
    "        neuro_in.append(neuro_out[::-1][0])\n",
    "    neuro_out[-1] = output_dim\n",
    "    neuro_in = neuro_in [:-1]\n",
    "    modules_enc = []\n",
    "    for i in range(nb_couche_lin):\n",
    "        modules_enc.append(ModuleLineaire(neuro_in[i], neuro_out[i], plage_biais, facteur_norma))\n",
    "        modules_enc.append(activations_enc[i])\n",
    "        \n",
    "\n",
    "    modules_dec = []\n",
    "    #neuro_out = neuro_out[::-1]\n",
    "    neuro_in = neuro_in[::-1]\n",
    "    neuro_out = neuro_out[::-1]\n",
    "    for i in range(nb_couche_lin):\n",
    "        modules_dec.append(ModuleLineaire(neuro_out[i], neuro_in[i], plage_biais, facteur_norma))\n",
    "        modules_dec.append(activations_dec[i])\n",
    "        \n",
    "    \n",
    "    network = Sequentiel(modules_enc + modules_dec)\n",
    "    BCE = BinaryCrossEntropie()\n",
    "    auto_encodeur_usps = AutoEncodeur(network, BCE)\n",
    "    auto_encodeur_usps.optimisation(X_train, X_train, batch_size, epochs,eps)\n",
    "    \n",
    "    return auto_encodeur_usps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimisation_nb_layers(X_train,Y_train,liste_layers,output_dim,facteur_norma,plage_biais,batch_size,epochs,random_activ):\n",
    "    shape = X_train.shape[1]\n",
    "    TanH = ModuleTanH()\n",
    "    sigmoide = ModuleSigmoide()\n",
    "    activ = np.array([TanH,sigmoide])\n",
    "    scores = []\n",
    "    act_s = []\n",
    "    opt_s = []\n",
    "    \n",
    "    \n",
    "  \n",
    "    for nb_layer in liste_layers:\n",
    "        print(\"Optimisation nb_layer  : \",nb_layer)\n",
    "        \n",
    "        if random_activ:\n",
    "            np.random.seed(42)\n",
    "            activations_enc = np.random.randint(0,2,size=nb_layer)\n",
    "            activations_dec = np.random.randint(0,2,size=nb_layer)\n",
    "\n",
    "            activ_enc = activ[activations_enc]\n",
    "            activ_dec = activ[activations_dec]\n",
    "\n",
    "            activ_layer = [activ_enc,activ_dec]\n",
    "        else:\n",
    "            activations_enc = [TanH for _ in range(nb_layer)]\n",
    "            activations_dec = [TanH for _ in range(nb_layer - 1)]\n",
    "            activations_dec.append(sigmoide)\n",
    "            activ_layer = [activations_enc,activations_dec]\n",
    "        \n",
    "        opt = create_auto_encodeur(X_train,output_dim, nb_layer,activ_layer, facteur_norma, plage_biais, batch_size, epochs,eps=1e-4)\n",
    "        \n",
    "        latent_repre = opt.encode(X_train)\n",
    "        X_train_hat = opt.decode(latent_repre)\n",
    "        \n",
    "        scores.append(evaluation([X_train_hat],Y_train)[0])\n",
    "        act_s.append(activ_layer)\n",
    "        opt_s.append(opt)\n",
    "        \n",
    "        \n",
    "    return scores,act_s,opt_s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimisation_network(X_train,Y_train,X_test,Y_test,liste_layers,liste_dim_espace_latent,batch_size,epochs,random_activ=False,verbose=False):\n",
    "    \n",
    "    facteur_norma = 0.8\n",
    "    plage_biais = (0,1)\n",
    "    resultat_train = {}\n",
    "    scores_test = []\n",
    "    for output_dim in liste_dim_espace_latent :\n",
    "        print(\"Optimisation de la dimension de l'espace latent : \",output_dim)\n",
    "        resultat_train[\"Espace latent \" +str(output_dim)] = optimisation_nb_layers(X_train,Y_train,liste_layers,output_dim,facteur_norma,plage_biais,batch_size,epochs,random_activ)\n",
    "        print()\n",
    "        l = []\n",
    "        for i,nb_layer in enumerate(liste_layers):\n",
    "            opt = resultat_train['Espace latent '+str(output_dim)][2][i]\n",
    "            latent_repre = opt.encode(X_test)\n",
    "            X_test_hat = opt.decode(latent_repre)\n",
    "\n",
    "            l.append(evaluation([X_test_hat],Y_test)[0])\n",
    "        scores_test.append(l)\n",
    "    \n",
    "    LDS = [\"LD\"+str(output_dim) for output_dim in liste_dim_espace_latent]\n",
    "    \n",
    "    if verbose:\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.subplot(1,2,1)\n",
    "        for i, output_dim in enumerate(liste_dim_espace_latent) :\n",
    "            \n",
    "            scores = resultat_train['Espace latent '+str(output_dim)][0]\n",
    "            plt.title(\"Accuracy du train selon l'espace latent \")\n",
    "            plt.xlabel(\"nombre de layers\")\n",
    "            plt.ylabel(\"accuracy\")\n",
    "            plt.plot(liste_layers,scores)\n",
    "        plt.legend(LDS)\n",
    "        \n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "            \n",
    "        for i, output_dim in enumerate(liste_dim_espace_latent) :\n",
    "\n",
    "            activs = resultat_train['Espace latent '+str(output_dim)][1]\n",
    "            plt.title(\"Accuracy du train selon l'espace latent\")\n",
    "            plt.xlabel(\"nombre de layers\")\n",
    "            plt.ylabel(\"accuracy\")\n",
    "            plt.plot(liste_layers,scores_test[i])\n",
    "\n",
    "        plt.legend(LDS)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    maximums = []\n",
    "    for i, output_dim in enumerate(liste_dim_espace_latent) :\n",
    "        \n",
    "        scores , activations ,_ = resultat_train['Espace latent '+str(output_dim)]\n",
    "        maximums.append(np.max(scores))\n",
    "        \n",
    "    \n",
    "    dim_espace_latent_optimal = liste_dim_espace_latent[np.argmax(maximums)]\n",
    "    scores = resultat_train['Espace latent '+str(dim_espace_latent_optimal)][0]\n",
    "    nb_layer_optimal = liste_layers[np.argmax(scores)]\n",
    "    nb_layer_optimal_ind = np.argmax(scores)\n",
    "    activ_optimal = resultat_train['Espace latent '+str(dim_espace_latent_optimal)][1][nb_layer_optimal_ind]\n",
    "    \n",
    "    network_optimal = create_auto_encodeur(X_train,dim_espace_latent_optimal, nb_layer_optimal,activ_optimal, facteur_norma, plage_biais, batch_size, epochs)\n",
    "    \n",
    "    print(\"Auto encodeur optimal : \")\n",
    "    print(\"Nombre layer optimal : \",nb_layer_optimal)\n",
    "    print(\"Dimension optimale de l'espace latent : \",dim_espace_latent_optimal)\n",
    "    print(\"Fonctions d'activation prises : \")\n",
    "    print([str(a) for a in activ_optimal[0]])\n",
    "    print([str(a) for a in activ_optimal[1]])\n",
    "        \n",
    "    return network_optimal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc543c",
   "metadata": {},
   "source": [
    "# 1. Donn√©es USPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ec4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pkl.load(open(\"data/usps.pkl\",'rb'))\n",
    "\n",
    "X_train = data[\"X_train\"]\n",
    "Y_train = data[\"Y_train\"]\n",
    "X_test = data[\"X_test\"]\n",
    "Y_test = data[\"Y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9aa78",
   "metadata": {},
   "source": [
    "## Optimisation des hyperparametres de l'auto encodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a2f9c2",
   "metadata": {},
   "source": [
    "### Fonctions d'activation du reseau d'une maniere aleatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d8f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la dimension de l'espace latent :  2\n",
      "Optimisation nb_layer  :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:10<00:00,  1.40s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:23<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation nb_layer  :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:23<00:00,  1.67s/it]\n",
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 27/50 [00:11<00:10,  2.23it/s]"
     ]
    }
   ],
   "source": [
    "liste_layers = np.array([2,3])\n",
    "liste_dim_espace_latent = np.array([2,8])\n",
    "batch_size = 50\n",
    "epochs = 50\n",
    "\n",
    "auto_encodeur_optimal_rand = optimisation_network(X_train,Y_train,X_test,Y_test,\n",
    "                                        liste_layers,liste_dim_espace_latent,batch_size,epochs,\n",
    "                                        random_activ=True,verbose=True)\n",
    "\n",
    "\n",
    "net_to_graph(network, net_name=\"networks_images/network_MC_alphabets\", horizontal=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b204ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_layer_optimal = len(auto_encodeur_optimal_rand.layers_encodeur) // 2\n",
    "latent_repre = auto_encodeur_optimal_rand.encode(X_train)\n",
    "dim_latent_optimale = latent_repre.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc03d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_repre = auto_encodeur_optimal_rand.encode(X_train)\n",
    "X_train_hat = auto_encodeur_optimal_rand.decode(latent_repre)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "sample = np.random.randint(0,100)\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" original\")\n",
    "    image = X_train[Y_train == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" reconstruit\")\n",
    "    image = X_train_hat[Y_train == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43acb3c",
   "metadata": {},
   "source": [
    "### Affichage des donn√©es USPS train induites de l'espace latent apres une t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22dd92",
   "metadata": {},
   "source": [
    "- Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691faab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, init='pca',n_iter=500, verbose=0)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "\n",
    "plt.subplot(131)\n",
    "repre_2D = tsne.fit_transform(X_train)\n",
    "plt.title(\"donn√©es USPS train d'origine \")\n",
    "plt.scatter(repre_2D[:,0],repre_2D[:,1],c=Y_train)\n",
    "\n",
    "plt.subplot(132)\n",
    "repre_2D = tsne.fit_transform(latent_repre)\n",
    "plt.title(\"l'espace latent des donn√©es USPS train\")\n",
    "plt.scatter(repre_2D[:,0],repre_2D[:,1],c=Y_train)\n",
    "\n",
    "plt.subplot(133)\n",
    "repre_2D = tsne.fit_transform(X_train_hat)\n",
    "plt.title(\"donn√©es USPS train induites de l'espace latent\")\n",
    "plt.scatter(repre_2D[:,0],repre_2D[:,1],c=Y_train)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842597be",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=0, max_iter=1000).fit(latent_repr)\n",
    "yhat = kmeans.labels_\n",
    "\n",
    "latent_repr = wine_auto_encodeur.encode(wine_data_norm)\n",
    "tsne = TSNE(n_components=2, init='pca',n_iter=500, verbose=0)\n",
    "latent_repre_tsne = tsne.fit_transform(latent_repr)\n",
    "\n",
    "colors = {i: plt.cm.tab10(i) for i in range(10)}\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "for c in range(10):\n",
    "    plt.scatter(latent_repre_tsne[:,0][labels == c],latent_repre_tsne[:,1][labels == c],color=colors[c],label=f\"classe {c}\")\n",
    "    \n",
    "plt.title(\"t-SNE sur l'espace latent avec les labels d'origine\")\n",
    "plt.legend()\n",
    "plt.subplot(122)\n",
    "\n",
    "y_cluster_pred = np.zeros(len(latent_repr))\n",
    "indices = np.arange(len(latent_repr))\n",
    "\n",
    "for cluster in range(10):\n",
    "\n",
    "    vals, counts = np.unique(labels[yhat == cluster], return_counts=True)\n",
    "    val_maj = vals[np.argmax(counts)]\n",
    "    \n",
    "    y_cluster_pred[indices[yhat == cluster]] = val_maj\n",
    "    \n",
    "    \n",
    "colors = {i: plt.cm.tab10(i) for i in range(10)}\n",
    "for c in range(10):\n",
    "    plt.scatter(latent_repre_tsne[:,0][y_cluster_pred == c],latent_repre_tsne[:,1][y_cluster_pred == c],color=colors[c],label=f\"classe {c}\")\n",
    "    \n",
    "plt.title(\"t-SNE sur l'espace latent avec les labels de kmeans\")\n",
    "plt.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881206a",
   "metadata": {},
   "source": [
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb48f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_repre_test = auto_encodeur_optimal_rand.encode(X_test)\n",
    "X_test_hat = auto_encodeur_optimal_rand.decode(latent_repre_test)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "sample = np.random.randint(0,100)\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" original\")\n",
    "    image = X_test[Y_test == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" reconstruit\")\n",
    "    image = X_test_hat[Y_test == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d36f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, init='pca',n_iter=500, verbose=0)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "\n",
    "plt.subplot(131)\n",
    "repre_2D = tsne.fit_transform(X_test)\n",
    "plt.title(\"donn√©es USPS test d'origine \")\n",
    "plt.scatter(repre_2D[:,0],repre_2D[:,1],c=Y_test)\n",
    "\n",
    "plt.subplot(132)\n",
    "repre_2D = tsne.fit_transform(latent_repre_test)\n",
    "plt.title(\"l'espace latent des donn√©es USPS test\")\n",
    "plt.scatter(repre_2D[:,0],repre_2D[:,1],c=Y_test)\n",
    "\n",
    "plt.subplot(133)\n",
    "repre_2D = tsne.fit_transform(X_test_hat)\n",
    "plt.title(\"donn√©es USPS test induites de l'espace latent\")\n",
    "plt.scatter(repre_2D[:,0],repre_2D[:,1],c=Y_test)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40f1fb4",
   "metadata": {},
   "source": [
    "### Evaluation de l'espace latent sur un reseau de neurones multi-classe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8e7d2",
   "metadata": {},
   "source": [
    "- Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae857fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [X_train,latent_repre,X_train_hat]\n",
    "evaluation(datas,Y_train,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb2eda",
   "metadata": {},
   "source": [
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [X_test,latent_repre_test,X_test_hat]\n",
    "evaluation(datas,Y_test,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb54e1",
   "metadata": {},
   "source": [
    "### PCA sur l'espace latent appris par reseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a7fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_1 = PCA(n_components=30,random_state=96)\n",
    "pca_1.fit(X_train_hat)\n",
    "\n",
    "pca_2 = PCA(n_components=30,random_state=96)\n",
    "pca_2.fit(X_train)\n",
    "\n",
    "pca_3 = PCA(n_components=dim_latent_optimale,random_state=96)\n",
    "pca_3.fit(latent_repre)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(np.cumsum(pca_1.explained_variance_ratio_))\n",
    "plt.plot(np.cumsum(pca_2.explained_variance_ratio_))\n",
    "plt.legend([\"X_train_hat\",\"X_train\"])\n",
    "plt.title(\"PCA - Taux informations captur√©\")\n",
    "plt.xlabel(\"Nombre de dimensions\")\n",
    "plt.ylabel(\"Taux de variance captur√©e\")\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(np.cumsum(pca_3.explained_variance_ratio_))\n",
    "plt.title(\"PCA - Taux informations captur√© sur la representation latente\")\n",
    "plt.xlabel(\"Nombre de dimensions\")\n",
    "plt.ylabel(\"Taux de variance captur√©e\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "quantite_info_apres_decodage = pca_1.explained_variance_ratio_[:8]\n",
    "quantite_info_espace_latent = pca_3.explained_variance_ratio_\n",
    "print(\"La quantit√© d'information moyenne perdue pendant le decodage : \",\n",
    "      np.abs( quantite_info_apres_decodage - quantite_info_espace_latent ).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdb01ac",
   "metadata": {},
   "source": [
    "- Le taux d'informations captur√© sur la representation latente converge vers un taux de 100% au bout de 6 dimensions . \n",
    "- On remarque aussi que le taux d'informations augmente plutot rapidement au fil des dimensions , ce qui veut dire que l'encodage de l'information est plutot pas mal.\n",
    "- Conclusion : la dimension ideale d'arriv√©e de l'encodeur peut etre fix√© √† 6 .\n",
    "\n",
    "\n",
    "- Le taux d'informations recuper√© sur les donn√©es issues de l'espace latent augmente plus rapidement par rapport √† celles des donn√©es de l'espace d'origine.  \n",
    "- Le taux d'informations recuper√© sur les donn√©es issues de l'espace latent arriv√© √† 100% au bout de 30 dimensions contrairement √† celles des donn√©es de l'espace d'origine qui arrve uniquement √† 80% de l'informations.\n",
    "- la difference de la quantit√© d'informations recuper√©s dans l'espace latent et celle du decodage de cet espace doit etre minimale pour pouvoir dire que le decodage a pu garder l'information de l'espace latent ( l'information de l'espace d'origine reduites en dimension )  \n",
    "- Un bon espace latent (regroupant le max d'informations de l'espace d'origine) permet de decoder de nouvelles images avec un pattern de chiffre parfait ( prend en compte le pattern de tous les chiffres de la classe y compris les images bruit√©s).\n",
    "- les donn√©es USPS d'origine pr√©sentent une quantit√© de bruit et de la redondance, ce qui peut brouiller la structure sous-jacente des donn√©es. Dans ce cas, la compression de dimension (l'encodage) aide √† √©liminer une partie du bruit et de la redondance, ce qui peut conduire √† une PCA de meilleure qualit√© pour la repr√©sentation de dimension r√©duite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_hat_pca = pca_1.transform(X_train_hat)\n",
    "X_train_pca = pca_2.transform(X_train)\n",
    "latent_repre_pca = pca_3.transform(latent_repre)\n",
    "\n",
    "plt.figure(figsize=(20,7))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.scatter(X_train_hat_pca[:,0],X_train_hat_pca[:,1],c=Y_train)\n",
    "plt.title(\"Nuage de points X_Train_Hat sur deux dimension\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(X_train_pca[:,0],X_train_pca[:,1],c=Y_train)\n",
    "plt.title(\"Nuage de points X_Train sur deux dimension\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(latent_repre_pca[:,0],latent_repre_pca[:,1],c=Y_train)\n",
    "plt.title(\"Nuage de points representation latent sur deux dimension\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "fig.add_subplot(projection='3d')\n",
    "\n",
    "plt.title(\"Nuage de points 3D X_Train_Hat\")\n",
    "plt.scatter(X_train_hat_pca[:,0],X_train_hat_pca[:,1],X_train_hat_pca[:,2],c=Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "fig.add_subplot(projection='3d')\n",
    "\n",
    "plt.title(\"Nuage de points 3D X_train\")\n",
    "plt.scatter(X_train_pca[:,0],X_train_pca[:,1],X_train_pca[:,2],c=Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4fdbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "fig.add_subplot(projection='3d')\n",
    "\n",
    "plt.title(\"Nuage de points 3D espace latent\")\n",
    "plt.scatter(latent_repre_pca[:,0],latent_repre_pca[:,1],latent_repre_pca[:,2],c=Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b64eac1",
   "metadata": {},
   "source": [
    "### Impact du batch sur l'apprentissage de l'espace latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = X_train.shape[1]\n",
    "nb_layers = nb_layer_optimal\n",
    "output_dim = dim_latent_optimale\n",
    "\n",
    "size = len(auto_encodeur_optimal_rand.layers_encodeur)\n",
    "auto_encodeur_optimal_rand.layers_encodeur = np.array(auto_encodeur_optimal_rand.layers_encodeur)\n",
    "indices = [i for i in range(size) if i % 2 == 1 ]\n",
    "activ_enc = auto_encodeur_optimal_rand.layers_encodeur[indices]\n",
    "\n",
    "auto_encodeur_optimal_rand.layers_decodeur = np.array(auto_encodeur_optimal_rand.layers_decodeur)\n",
    "indices = [i for i in range(size) if i % 2 == 1 ]\n",
    "activ_dec = list(auto_encodeur_optimal_rand.layers_decodeur[indices])\n",
    "\n",
    "activations = [ activ_enc , activ_dec]\n",
    "\n",
    "facteur_norma = 0.8\n",
    "plage_biais = (0,1)\n",
    "epochs = 50\n",
    "liste_batch_size = [50,300,500]\n",
    "liste_auto_encodeurs = []\n",
    "\n",
    "for batch_size in liste_batch_size:\n",
    "\n",
    "    liste_auto_encodeurs.append(create_auto_encodeur(X_train,output_dim, nb_layers,activations, \n",
    "                                                 facteur_norma, plage_biais, batch_size, epochs,eps=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be630ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, init='pca',n_iter=500, verbose=0)\n",
    "\n",
    "for i,auto_encodeur in enumerate(liste_auto_encodeurs):\n",
    "    \n",
    "    latent_repre = auto_encodeur.encode(X_train)\n",
    "    X_train_hat = auto_encodeur.decode(latent_repre)\n",
    "    \n",
    "    X_train_hat_pca = pca_1.transform(X_train_hat)\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    repre_2D = tsne.fit_transform(X_train_hat)\n",
    "    plt.title(\"donn√©es USPS train induites de l'espace latent\")\n",
    "    plt.scatter(repre_2D[:,0],repre_2D[:,1],c=Y_train)\n",
    "    plt.legend([\"Batch_size : \"+str(liste_batch_size[i])])\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title(\"Image reconstruit\")\n",
    "    image = X_train_hat[Y_train == 0][27].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "    plt.legend([\"Batch_size : \"+str(liste_batch_size[i])])\n",
    "    \n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title(\"Nuage de points X_Train_Hat\")\n",
    "    plt.scatter(X_train_hat_pca[:,0],X_train_hat_pca[:,1],c=Y_train)\n",
    "    plt.legend([\"Batch_size : \"+str(liste_batch_size[i])])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc1ee48",
   "metadata": {},
   "source": [
    "## Debruitage de donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47deeb15",
   "metadata": {},
   "source": [
    "#### Bruit poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f720ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamdas = [1e-3,1e-1,1,3]\n",
    "scores = []\n",
    "\n",
    "for i,lamda in enumerate(lamdas):\n",
    "    noise = generate_noise(X_train.shape[1],'poisson',lamda,1e-1)\n",
    "    X_train_noised = X_train + noise\n",
    "    \n",
    "    latent_repre_noised = auto_encodeur_optimal_rand.encode(X_train_noised)\n",
    "    X_train_noised_hat = auto_encodeur_optimal_rand.decode(latent_repre_noised)\n",
    "    sample = 1\n",
    "    plt.figure(figsize=(15,20))\n",
    "\n",
    "    plt.subplot(i+1,3,1)\n",
    "    plt.title(\"Original \")\n",
    "    image = X_train[sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "\n",
    "\n",
    "    plt.subplot(i+1,3,2)\n",
    "    plt.title(\"Original noised - poisson(\"+str(lamda)+\")\")\n",
    "    image = X_train_noised[sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "\n",
    "\n",
    "    plt.subplot(i+1,3,3)\n",
    "    plt.title(\"Reconstruite - poisson(\"+str(lamda)+\")\")\n",
    "    image = X_train_noised_hat[sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    datas = [X_train_noised,latent_repre_noised,X_train_noised_hat]\n",
    "    scores.append(evaluation(datas,Y_train,verbose=False)[0][2])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4277a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "plt.title(\"Score sur les donn√©es en fonction du lambda\")\n",
    "plt.xlabel(\"Lambda - Bruit poisson\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Train\"])\n",
    "plt.plot(lamdas,scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Bruit gaussien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4074ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [1e-3,1e-1,0.5,3]\n",
    "stds = [1e-3,1e-1,0.5,3]\n",
    "scores = []\n",
    "\n",
    "\n",
    "for i,mean in enumerate(means):\n",
    "    noise = generate_noise(X_train.shape[1],'gaussien',mean,stds[i])\n",
    "    X_train_noised = X_train + noise\n",
    "    \n",
    "    latent_repre_noised = auto_encodeur_optimal_rand.encode(X_train_noised)\n",
    "    X_train_noised_hat = auto_encodeur_optimal_rand.decode(latent_repre_noised)\n",
    "    sample = 1\n",
    "    plt.figure(figsize=(15,20))\n",
    "\n",
    "    plt.subplot(i+1,3,1)\n",
    "    plt.title(\"Original \")\n",
    "    image = X_train[sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "\n",
    "\n",
    "    plt.subplot(i+1,3,2)\n",
    "    plt.title(\"Original noised - normal(\"+str(mean)+\",\"+str(stds[i])+\")\")\n",
    "    image = X_train_noised[sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "\n",
    "\n",
    "    plt.subplot(i+1,3,3)\n",
    "    plt.title(\"Reconstruite - normal(\"+str(mean)+\",\"+str(stds[i])+\")\")\n",
    "    image = X_train_noised_hat[sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    datas = [X_train_noised,latent_repre_noised,X_train_noised_hat]\n",
    "    scores.append(evaluation(datas,Y_train,verbose=False)[0][2])\n",
    "    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "plt.title(\"Score sur les donn√©es en fonction de la moyenne\")\n",
    "plt.xlabel(\"Mean - Bruit gaussien\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Train\"])\n",
    "plt.plot(means,scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa540e5",
   "metadata": {},
   "source": [
    "- Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = generate_noise(X_train.shape[1],'poisson',1e-1,1e-1)\n",
    "X_train_noised = X_train + noise\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Image USPS originale\")\n",
    "plt.imshow(X_train[10].reshape((16,16)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Image USPS bruit√©\")\n",
    "plt.imshow(X_train_noised[10].reshape((16,16)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd0fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "sample = np.random.randint(0,50)\n",
    "\n",
    "latent_repre_noised = auto_encodeur_optimal_rand.encode(X_train_noised)\n",
    "X_train_noised_hat = auto_encodeur_optimal_rand.decode(latent_repre_noised)\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" original\")\n",
    "    image = X_train[Y_train == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" original noised\")\n",
    "    image = X_train_noised[Y_train == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" reconstruit\")\n",
    "    image = X_train_noised_hat[Y_train == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [X_train_noised,latent_repre_noised,X_train_noised_hat]\n",
    "evaluation(datas,Y_train,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc5baa4",
   "metadata": {},
   "source": [
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = generate_noise(X_test.shape[1],'poisson',1e-2,1e-1)\n",
    "X_test_noised = X_test + noise\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Image USPS originale\")\n",
    "plt.imshow(X_test[10].reshape((16,16)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Image USPS bruit√©\")\n",
    "plt.imshow(X_test_noised[10].reshape((16,16)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_repre_noised = auto_encodeur_optimal_rand.encode(X_test_noised)\n",
    "X_test_noised_hat = auto_encodeur_optimal_rand.decode(latent_repre_noised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee8af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "sample = np.random.randint(0,50)\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" original\")\n",
    "    image = X_test[Y_test == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" original noised\")\n",
    "    image = X_test_noised[Y_test == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" reconstruit\")\n",
    "    image = X_test_noised_hat[Y_test == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [X_test_noised,latent_repre_noised,X_test_noised_hat]\n",
    "evaluation(datas,Y_test,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f5b624",
   "metadata": {},
   "source": [
    "# 2. Donn√©es Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "data = load_wine()\n",
    "labels = data.target\n",
    "\n",
    "wine_data_norm = normalisation(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d82f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 3\n",
    "nb_layers = 2\n",
    "TanH = ModuleTanH()\n",
    "sig = ModuleSigmoide()\n",
    "activ_enc = [TanH for _ in range(2)]\n",
    "activ_dec = [TanH for _ in range(1)]\n",
    "activ_dec.append(sig)\n",
    "\n",
    "activations = [activ_enc , activ_dec]\n",
    "\n",
    "facteur_norma = 0.8\n",
    "plage_biais = (0,1)\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "wine_auto_encodeur = create_auto_encodeur(wine_data_norm,output_dim, nb_layers,activations, \n",
    "                                                 facteur_norma, plage_biais, batch_size, epochs,eps=1e-4)\n",
    "\n",
    "\n",
    "net_to_graph(wine_auto_encodeur.network, net_name=\"networks_images/network_AE_wine\", horizontal=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48208ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,7))\n",
    "\n",
    "pca = PCA(n_components=2,random_state=96)\n",
    "pca.fit(wine_data_norm)\n",
    "wine_data_norm_pca = pca.transform(wine_data_norm)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"PCA sur l'espace d'origine\")\n",
    "plt.scatter(wine_data_norm_pca[:,0],wine_data_norm_pca[:,1],c=labels)\n",
    "\n",
    "latent_repre = wine_auto_encodeur.encode(wine_data_norm)\n",
    "plt.subplot(122)\n",
    "plt.title(\"Espace latent\")\n",
    "plt.scatter(latent_repre[:,0],latent_repre[:,1],c=labels)\n",
    "\n",
    "\n",
    "latent_repre = wine_auto_encodeur.encode(wine_data_norm)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(wine_data_norm,labels)\n",
    "print(\"Accuracy sur l'espace d'origine : \",clf.score(wine_data_norm,labels))\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, max_iter=100).fit(wine_data_norm)\n",
    "yhat = kmeans.labels_\n",
    "\n",
    "print(\"Puerete du clustering sur l'espace d'origine: \",cluster_purity(yhat,labels))\n",
    "print()\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(latent_repre,labels)\n",
    "print(\"Accuracy sur l'espace latent : \",clf.score(latent_repre,labels))\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, max_iter=100).fit(latent_repre)\n",
    "yhat = kmeans.labels_\n",
    "\n",
    "print(\"Puerete du clustering sur l'espace latent : \",cluster_purity(yhat,labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb035df9",
   "metadata": {},
   "source": [
    "# 3. Donn√©es Cr√©dit Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec939e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data = pd.read_csv(\"data/CC GENERAL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80547b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86362e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprimer les valeurs NaN\n",
    "cc_data = cc_data.dropna()\n",
    "\n",
    "cc_data = np.array(cc_data)\n",
    "# supprimer la colonne 1\n",
    "cc_data = cc_data[:,1:]\n",
    "cc_data = cc_data.astype('float')\n",
    "y = np.random.randint(0,1,cc_data.shape[0])\n",
    "\n",
    "data_norm = normalisation(cc_data)\n",
    "\n",
    "data_norm = data_norm.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = data_norm.shape[1]\n",
    "nb_layers = 2\n",
    "output_dim = 7\n",
    "\n",
    "TanH = ModuleTanH()\n",
    "sig = ModuleSigmoide()\n",
    "activ_enc = [TanH for _ in range(nb_layers)]\n",
    "activ_dec = [TanH for _ in range(nb_layers - 1)]\n",
    "activ_dec.append(sig)\n",
    "\n",
    "activations = [activ_enc , activ_dec]\n",
    "\n",
    "\n",
    "facteur_norma = 0.8\n",
    "plage_biais = (0,1)\n",
    "batch_size = 50\n",
    "epochs = 50\n",
    "\n",
    "auto_encodeur_cc_optimal= create_auto_encodeur(data_norm,output_dim, nb_layers,activations, \n",
    "                                                 facteur_norma, plage_biais, batch_size, epochs,eps=1e-4)\n",
    "\n",
    "net_to_graph(auto_encodeur_cc_optimal.network, net_name=\"networks_images/network_AE_credit_card\", horizontal=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd8901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_repre = auto_encodeur_cc_optimal.encode(data_norm)\n",
    "data_norm_hat = auto_encodeur_cc_optimal.decode(latent_repre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b328267",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=0, max_iter=100).fit(data_norm_hat)\n",
    "\n",
    "plt.figure(figsize=(20,7))\n",
    "\n",
    "yhat = kmeans.labels_\n",
    "\n",
    "plt.subplot(121)\n",
    "tsne = TSNE(n_components=2, init='pca',n_iter=500, verbose=0)\n",
    "repre_2D = tsne.fit_transform(data_norm_hat)\n",
    "\n",
    "plt.title(\"Representation des donn√©es induites de l'espace latent apres une t-SNE sur CCData\")\n",
    "plt.scatter(repre_2D[:,0],repre_2D[:,1],c=yhat)\n",
    "\n",
    "plt.subplot(122)\n",
    "tsne = TSNE(n_components=2, init='pca',n_iter=500, verbose=0)\n",
    "repre_2D = tsne.fit_transform(latent_repre)\n",
    "\n",
    "plt.title(\"Representation de l'espace latent apres une t-SNE sur CCData\")\n",
    "plt.scatter(repre_2D[:,0],repre_2D[:,1],c=yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=17,random_state=96)\n",
    "pca.fit(data_norm)\n",
    "X_train_pca = pca.transform(data_norm)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,7))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X_train_pca[:,0],X_train_pca[:,1],c=yhat)\n",
    "plt.title(\"PCA sur les donn√©es originale CCdata\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"PCA sur l'espace latent des donn√©es CCdata\")\n",
    "plt.scatter(latent_repre[:,0],latent_repre[:,1],c=yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
