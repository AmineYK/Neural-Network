{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf42b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from modules import *\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from os import listdir\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import rand_score,adjusted_rand_score\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9568321",
   "metadata": {},
   "source": [
    "# Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f90651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(datas,labels,verbose=False):\n",
    "    titles = [\"X\",\"Representation Latente\",\"X_hat\"]\n",
    "    scores = []\n",
    "    puritities = []\n",
    "    rand_scores= []\n",
    "    for i,data in enumerate(datas):\n",
    "\n",
    "        shape = data.shape[1]\n",
    "        nb_classes = len(np.unique(labels))\n",
    "\n",
    "        neuro_i_1 = shape\n",
    "        neuro_o_1 = shape // 2\n",
    "        neuro_i_2 = shape // 2\n",
    "        neuro_o_2 = shape // 4\n",
    "        neuro_i_3 = shape // 4\n",
    "        neuro_o_3 = nb_classes\n",
    "        facteur_norma = 0.4\n",
    "        plage_biais = (0,1)\n",
    "        batch_size = 100\n",
    "        epochs = 50\n",
    "\n",
    "\n",
    "        facteur_norma = 0.8\n",
    "        lineaire_1 = ModuleLineaire(neuro_i_1 ,neuro_o_1 ,plage_biais,facteur_norma)\n",
    "        lineaire_2 = ModuleLineaire(neuro_i_2 ,neuro_o_2 ,plage_biais,facteur_norma)\n",
    "        lineaire_3 = ModuleLineaire(neuro_i_3 ,neuro_o_3 ,plage_biais,facteur_norma)\n",
    "        TanH = ModuleTanH()\n",
    "        sigmoide = ModuleSigmoide()\n",
    "        mseloss = MSELoss()\n",
    "        CE = CrossEntropieLoss(nb_classes)\n",
    "        softmax = SoftMax()\n",
    "\n",
    "\n",
    "        network_layers = [lineaire_1,TanH,lineaire_2,TanH,lineaire_3,softmax]\n",
    "        network = Sequentiel(network_layers)\n",
    "\n",
    "        if verbose : print(\"Optimisation de : \",titles[i])\n",
    "        opti = Optim(network,CE,1e-2)\n",
    "        opti.SGD(data,labels,batch_size,epochs)\n",
    "\n",
    "        score = opti.getNetwork().accuracy(data,labels) \n",
    "        scores.append(score)\n",
    "        if verbose : print(\"Accuracy sur les images issues de l'auto encodeur\",score)    \n",
    "        \n",
    "        kmeans = KMeans(n_clusters=10, random_state=0, max_iter=1000).fit(data)\n",
    "        yhat = kmeans.labels_\n",
    "        \n",
    "        \n",
    "        y_cluster_pred = np.zeros(len(data))\n",
    "        indices = np.arange(len(data))\n",
    "\n",
    "        for cluster in range(3):\n",
    "\n",
    "            vals, counts = np.unique(labels[yhat == cluster], return_counts=True)\n",
    "            val_maj = vals[np.argmax(counts)]\n",
    "\n",
    "            y_cluster_pred[indices[yhat == cluster]] = val_maj\n",
    "        \n",
    "        purete = cluster_purity(labels,y_cluster_pred)\n",
    "        puritities.append(purete)\n",
    "        \n",
    "        rs  = rand_score(labels,y_cluster_pred)\n",
    "        rand_scores.append(rs)\n",
    "        \n",
    "        if verbose : \n",
    "            print(\"Pureté du clustering : \",purete)\n",
    "            print(\"Rand score  : \",rs)\n",
    "            print(\"Adjusted Rand Score : \",adjusted_rand_score(labels,y_cluster_pred))\n",
    "            print()\n",
    "\n",
    "        \n",
    "        \n",
    "    return scores,puritities,rand_scores\n",
    "        \n",
    "        \n",
    "        \n",
    "def optimisation_espace_latent(data,labels,dim_espace_latent_list,eps,verbose=False):\n",
    "    \n",
    "    nb_classes = len(np.unique(labels))\n",
    "    n_samples = data.shape[0]\n",
    "    scores = []\n",
    "    puritities = []\n",
    "    rand_scores= []\n",
    "    \n",
    "    \n",
    "    for dim_espace_latent in dim_espace_latent_list:\n",
    "        \n",
    "\n",
    "        neuro_i_1 = 256\n",
    "        neuro_o_1 = 160\n",
    "        neuro_i_2 = 160\n",
    "        neuro_o_2 = 120\n",
    "        neuro_i_3 = 120\n",
    "        neuro_o_3 = 60\n",
    "        neuro_i_4 = 60\n",
    "        neuro_o_4 = dim_espace_latent\n",
    "        \n",
    "        facteur_norma = 0.4\n",
    "        plage_biais = (0,1)\n",
    "        batch_size = 100\n",
    "        epochs = 100\n",
    "        nb_couches = 4\n",
    "\n",
    "\n",
    "        facteur_norma = 0.8\n",
    "\n",
    "\n",
    "        lineaire_1_enc = ModuleLineaire(neuro_i_1 ,neuro_o_1 ,plage_biais,facteur_norma)\n",
    "        lineaire_2_enc = ModuleLineaire(neuro_i_2 ,neuro_o_2 ,plage_biais,facteur_norma)\n",
    "        lineaire_3_enc = ModuleLineaire(neuro_i_3 ,neuro_o_3 ,plage_biais,facteur_norma)\n",
    "        lineaire_4_enc = ModuleLineaire(neuro_i_4 ,neuro_o_4 ,plage_biais,facteur_norma)\n",
    "\n",
    "        lineaire_1_dec = ModuleLineaire(neuro_o_1 ,neuro_i_1 ,plage_biais,facteur_norma)\n",
    "        lineaire_2_dec = ModuleLineaire(neuro_o_2 ,neuro_i_2 ,plage_biais,facteur_norma)\n",
    "        lineaire_3_dec = ModuleLineaire(neuro_o_3 ,neuro_i_3 ,plage_biais,facteur_norma)\n",
    "        lineaire_4_dec = ModuleLineaire(neuro_o_4 ,neuro_i_4 ,plage_biais,facteur_norma)\n",
    "\n",
    "        TanH = ModuleTanH()\n",
    "        sigmoide = ModuleSigmoide()\n",
    "        BCE = BinaryCrossEntropie()\n",
    "\n",
    "\n",
    "        network_layers = [lineaire_1_enc,TanH,lineaire_2_enc,TanH,lineaire_3_enc,TanH,lineaire_4_enc,TanH,\n",
    "                          lineaire_4_dec,TanH,lineaire_3_dec,TanH,lineaire_2_dec,TanH,lineaire_1_dec,sigmoide]\n",
    "        network = Sequentiel(network_layers)\n",
    "\n",
    "        auto_encodeur_usps = AutoEncodeur(network,BCE)\n",
    "\n",
    "        \n",
    "        if verbose : print(\"Dimension espace latent : \",dim_espace_latent)\n",
    "        \n",
    "        auto_encodeur_usps.optimisation(data,labels,batch_size,epochs,eps,False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        latent_repre = auto_encodeur_usps.encode(data)\n",
    "        \n",
    "        datas = [latent_repre]\n",
    "        score,purete,rand_score = evaluation(datas,labels,verbose)\n",
    "        scores.append(score)\n",
    "        puritities.append(purete)\n",
    "        rand_scores.append(rand_score)\n",
    "        \n",
    "    return scores , puritities , rand_scores\n",
    "\n",
    "\n",
    "\n",
    "def create_auto_encodeur(X_train,output_dim, nb_couche_lin,activations, facteur_norma, plage_biais, batch_size, epochs,eps=1e-4):\n",
    "    np.random.seed(42)\n",
    "    input_dim = X_train.shape[1]\n",
    "    n_samples = X_train.shape[0]\n",
    "    neuro_in = [input_dim]\n",
    "    neuro_out = []\n",
    "    activations_enc = activations[0]\n",
    "    activations_dec = activations[1]\n",
    "    assert len(activations_enc) == nb_couche_lin and len(activations_dec) == nb_couche_lin\n",
    "    for i in range(0,nb_couche_lin) : \n",
    "        neuro_out.append(neuro_in[::-1][0] // 2)\n",
    "        neuro_in.append(neuro_out[::-1][0])\n",
    "    neuro_out[-1] = output_dim\n",
    "    neuro_in = neuro_in [:-1]\n",
    "    modules_enc = []\n",
    "    for i in range(nb_couche_lin):\n",
    "        modules_enc.append(ModuleLineaire(neuro_in[i], neuro_out[i], plage_biais, facteur_norma))\n",
    "        modules_enc.append(activations_enc[i])\n",
    "        \n",
    "\n",
    "    modules_dec = []\n",
    "    #neuro_out = neuro_out[::-1]\n",
    "    neuro_in = neuro_in[::-1]\n",
    "    neuro_out = neuro_out[::-1]\n",
    "    for i in range(nb_couche_lin):\n",
    "        modules_dec.append(ModuleLineaire(neuro_out[i], neuro_in[i], plage_biais, facteur_norma))\n",
    "        modules_dec.append(activations_dec[i])\n",
    "        \n",
    "    \n",
    "    network = Sequentiel(modules_enc + modules_dec)\n",
    "    BCE = BinaryCrossEntropie()\n",
    "    auto_encodeur_usps = AutoEncodeur(network, BCE)\n",
    "    auto_encodeur_usps.optimisation(X_train, X_train, batch_size, epochs,eps)\n",
    "    \n",
    "    return auto_encodeur_usps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimisation_nb_layers(X_train,Y_train,liste_layers,output_dim,facteur_norma,plage_biais,batch_size,epochs,random_activ):\n",
    "    shape = X_train.shape[1]\n",
    "    TanH = ModuleTanH()\n",
    "    sigmoide = ModuleSigmoide()\n",
    "    activ = np.array([TanH,sigmoide])\n",
    "    scores = []\n",
    "    act_s = []\n",
    "    opt_s = []\n",
    "    \n",
    "    \n",
    "  \n",
    "    for nb_layer in liste_layers:\n",
    "        print(\"Optimisation nb_layer  : \",nb_layer)\n",
    "        \n",
    "        if random_activ:\n",
    "            np.random.seed(42)\n",
    "            activations_enc = np.random.randint(0,2,size=nb_layer)\n",
    "            activations_dec = np.random.randint(0,2,size=nb_layer)\n",
    "\n",
    "            activ_enc = activ[activations_enc]\n",
    "            activ_dec = activ[activations_dec]\n",
    "\n",
    "            activ_layer = [activ_enc,activ_dec]\n",
    "        else:\n",
    "            activations_enc = [TanH for _ in range(nb_layer)]\n",
    "            activations_dec = [TanH for _ in range(nb_layer - 1)]\n",
    "            activations_dec.append(sigmoide)\n",
    "            activ_layer = [activations_enc,activations_dec]\n",
    "        \n",
    "        opt = create_auto_encodeur(X_train,output_dim, nb_layer,activ_layer, facteur_norma, plage_biais, batch_size, epochs,eps=1e-4)\n",
    "        \n",
    "        latent_repre = opt.encode(X_train)\n",
    "        X_train_hat = opt.decode(latent_repre)\n",
    "        \n",
    "        scores.append(evaluation([X_train_hat],Y_train)[0])\n",
    "        act_s.append(activ_layer)\n",
    "        opt_s.append(opt)\n",
    "        \n",
    "        \n",
    "    return scores,act_s,opt_s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimisation_network(X_train,Y_train,X_test,Y_test,liste_layers,liste_dim_espace_latent,batch_size,epochs,random_activ=False,verbose=False):\n",
    "    \n",
    "    facteur_norma = 0.8\n",
    "    plage_biais = (0,1)\n",
    "    resultat_train = {}\n",
    "    scores_test = []\n",
    "    for output_dim in liste_dim_espace_latent :\n",
    "        print(\"Optimisation de la dimension de l'espace latent : \",output_dim)\n",
    "        resultat_train[\"Espace latent \" +str(output_dim)] = optimisation_nb_layers(X_train,Y_train,liste_layers,output_dim,facteur_norma,plage_biais,batch_size,epochs,random_activ)\n",
    "        print()\n",
    "        l = []\n",
    "        for i,nb_layer in enumerate(liste_layers):\n",
    "            opt = resultat_train['Espace latent '+str(output_dim)][2][i]\n",
    "            latent_repre = opt.encode(X_test)\n",
    "            X_test_hat = opt.decode(latent_repre)\n",
    "\n",
    "            l.append(evaluation([X_test_hat],Y_test)[0])\n",
    "        scores_test.append(l)\n",
    "    \n",
    "    LDS = [\"LD\"+str(output_dim) for output_dim in liste_dim_espace_latent]\n",
    "    \n",
    "    if verbose:\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.subplot(1,2,1)\n",
    "        for i, output_dim in enumerate(liste_dim_espace_latent) :\n",
    "            \n",
    "            scores = resultat_train['Espace latent '+str(output_dim)][0]\n",
    "            plt.title(\"Accuracy du train selon l'espace latent \")\n",
    "            plt.xlabel(\"nombre de layers\")\n",
    "            plt.ylabel(\"accuracy\")\n",
    "            plt.plot(liste_layers,scores)\n",
    "        plt.legend(LDS)\n",
    "        \n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "            \n",
    "        for i, output_dim in enumerate(liste_dim_espace_latent) :\n",
    "\n",
    "            activs = resultat_train['Espace latent '+str(output_dim)][1]\n",
    "            plt.title(\"Accuracy du train selon l'espace latent\")\n",
    "            plt.xlabel(\"nombre de layers\")\n",
    "            plt.ylabel(\"accuracy\")\n",
    "            plt.plot(liste_layers,scores_test[i])\n",
    "\n",
    "        plt.legend(LDS)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    maximums = []\n",
    "    for i, output_dim in enumerate(liste_dim_espace_latent) :\n",
    "        \n",
    "        scores , activations ,_ = resultat_train['Espace latent '+str(output_dim)]\n",
    "        maximums.append(np.max(scores))\n",
    "        \n",
    "    \n",
    "    dim_espace_latent_optimal = liste_dim_espace_latent[np.argmax(maximums)]\n",
    "    scores = resultat_train['Espace latent '+str(dim_espace_latent_optimal)][0]\n",
    "    nb_layer_optimal = liste_layers[np.argmax(scores)]\n",
    "    nb_layer_optimal_ind = np.argmax(scores)\n",
    "    activ_optimal = resultat_train['Espace latent '+str(dim_espace_latent_optimal)][1][nb_layer_optimal_ind]\n",
    "    \n",
    "    network_optimal = create_auto_encodeur(X_train,dim_espace_latent_optimal, nb_layer_optimal,activ_optimal, facteur_norma, plage_biais, batch_size, epochs)\n",
    "    \n",
    "    print(\"Auto encodeur optimal : \")\n",
    "    print(\"Nombre layer optimal : \",nb_layer_optimal)\n",
    "    print(\"Dimension optimale de l'espace latent : \",dim_espace_latent_optimal)\n",
    "    print(\"Fonctions d'activation prises : \")\n",
    "    print([str(a) for a in activ_optimal[0]])\n",
    "    print([str(a) for a in activ_optimal[1]])\n",
    "        \n",
    "    return network_optimal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc543c",
   "metadata": {},
   "source": [
    "# 1. Données USPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ec4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pkl.load(open(\"data/usps.pkl\",'rb'))\n",
    "\n",
    "X_train = data[\"X_train\"]\n",
    "Y_train = data[\"Y_train\"]\n",
    "X_test = data[\"X_test\"]\n",
    "Y_test = data[\"Y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9aa78",
   "metadata": {},
   "source": [
    "## Optimisation des hyperparametres de l'auto encodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a2f9c2",
   "metadata": {},
   "source": [
    "### Fonctions d'activation du reseau d'une maniere aleatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d8f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation de la dimension de l'espace latent :  2\n",
      "Optimisation nb_layer  :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 50/50 [01:10<00:00,  1.40s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:23<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation nb_layer  :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 50/50 [01:23<00:00,  1.67s/it]\n",
      " 54%|███████████████████████▏                   | 27/50 [00:11<00:10,  2.23it/s]"
     ]
    }
   ],
   "source": [
    "liste_layers = np.array([2,3])\n",
    "liste_dim_espace_latent = np.array([2,8])\n",
    "batch_size = 50\n",
    "epochs = 50\n",
    "\n",
    "auto_encodeur_optimal_rand = optimisation_network(X_train,Y_train,X_test,Y_test,\n",
    "                                        liste_layers,liste_dim_espace_latent,batch_size,epochs,\n",
    "                                        random_activ=True,verbose=True)\n",
    "\n",
    "\n",
    "net_to_graph(network, net_name=\"networks_images/network_MC_alphabets\", horizontal=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b204ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_layer_optimal = len(auto_encodeur_optimal_rand.layers_encodeur) // 2\n",
    "latent_repre = auto_encodeur_optimal_rand.encode(X_train)\n",
    "dim_latent_optimale = latent_repre.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc03d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_repre = auto_encodeur_optimal_rand.encode(X_train)\n",
    "X_train_hat = auto_encodeur_optimal_rand.decode(latent_repre)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "sample = np.random.randint(0,100)\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" original\")\n",
    "    image = X_train[Y_train == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" reconstruit\")\n",
    "    image = X_train_hat[Y_train == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43acb3c",
   "metadata": {},
   "source": [
    "### Affichage des données USPS train induites de l'espace latent apres une t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22dd92",
   "metadata": {},
   "source": [
    "- Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691faab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, init='pca',n_iter=500, verbose=0)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "\n",
    "plt.subplot(131)\n",
    "repre_2D = tsne.fit_transform(X_train)\n",
    "plt.title(\"données USPS train d'origine \")\n",
    "plt.scatter(repre_2D[:,0],repre_2D[:,1],c=Y_train)\n",
    "\n",
    "plt.subplot(132)\n",
    "repre_2D = tsne.fit_transform(latent_repre)\n",
    "plt.title(\"l'espace latent des données USPS train\")\n",
    "plt.scatter(repre_2D[:,0],repre_2D[:,1],c=Y_train)\n",
    "\n",
    "plt.subplot(133)\n",
    "repre_2D = tsne.fit_transform(X_train_hat)\n",
    "plt.title(\"données USPS train induites de l'espace latent\")\n",
    "plt.scatter(repre_2D[:,0],repre_2D[:,1],c=Y_train)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842597be",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=0, max_iter=1000).fit(latent_repr)\n",
    "yhat = kmeans.labels_\n",
    "\n",
    "latent_repr = wine_auto_encodeur.encode(wine_data_norm)\n",
    "tsne = TSNE(n_components=2, init='pca',n_iter=500, verbose=0)\n",
    "latent_repre_tsne = tsne.fit_transform(latent_repr)\n",
    "\n",
    "colors = {i: plt.cm.tab10(i) for i in range(10)}\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "for c in range(10):\n",
    "    plt.scatter(latent_repre_tsne[:,0][labels == c],latent_repre_tsne[:,1][labels == c],color=colors[c],label=f\"classe {c}\")\n",
    "    \n",
    "plt.title(\"t-SNE sur l'espace latent avec les labels d'origine\")\n",
    "plt.legend()\n",
    "plt.subplot(122)\n",
    "\n",
    "y_cluster_pred = np.zeros(len(latent_repr))\n",
    "indices = np.arange(len(latent_repr))\n",
    "\n",
    "for cluster in range(10):\n",
    "\n",
    "    vals, counts = np.unique(labels[yhat == cluster], return_counts=True)\n",
    "    val_maj = vals[np.argmax(counts)]\n",
    "    \n",
    "    y_cluster_pred[indices[yhat == cluster]] = val_maj\n",
    "    \n",
    "    \n",
    "colors = {i: plt.cm.tab10(i) for i in range(10)}\n",
    "for c in range(10):\n",
    "    plt.scatter(latent_repre_tsne[:,0][y_cluster_pred == c],latent_repre_tsne[:,1][y_cluster_pred == c],color=colors[c],label=f\"classe {c}\")\n",
    "    \n",
    "plt.title(\"t-SNE sur l'espace latent avec les labels de kmeans\")\n",
    "plt.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881206a",
   "metadata": {},
   "source": [
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb48f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_repre_test = auto_encodeur_optimal_rand.encode(X_test)\n",
    "X_test_hat = auto_encodeur_optimal_rand.decode(latent_repre_test)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "sample = np.random.randint(0,100)\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" original\")\n",
    "    image = X_test[Y_test == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" reconstruit\")\n",
    "    image = X_test_hat[Y_test == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d36f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, init='pca',n_iter=500, verbose=0)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "\n",
    "plt.subplot(131)\n",
    "repre_2D = tsne.fit_transform(X_test)\n",
    "plt.title(\"données USPS test d'origine \")\n",
    "plt.scatter(repre_2D[:,0],repre_2D[:,1],c=Y_test)\n",
    "\n",
    "plt.subplot(132)\n",
    "repre_2D = tsne.fit_transform(latent_repre_test)\n",
    "plt.title(\"l'espace latent des données USPS test\")\n",
    "plt.scatter(repre_2D[:,0],repre_2D[:,1],c=Y_test)\n",
    "\n",
    "plt.subplot(133)\n",
    "repre_2D = tsne.fit_transform(X_test_hat)\n",
    "plt.title(\"données USPS test induites de l'espace latent\")\n",
    "plt.scatter(repre_2D[:,0],repre_2D[:,1],c=Y_test)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40f1fb4",
   "metadata": {},
   "source": [
    "### Evaluation de l'espace latent sur un reseau de neurones multi-classe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8e7d2",
   "metadata": {},
   "source": [
    "- Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae857fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [X_train,latent_repre,X_train_hat]\n",
    "evaluation(datas,Y_train,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb2eda",
   "metadata": {},
   "source": [
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [X_test,latent_repre_test,X_test_hat]\n",
    "evaluation(datas,Y_test,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb54e1",
   "metadata": {},
   "source": [
    "### PCA sur l'espace latent appris par reseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a7fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_1 = PCA(n_components=30,random_state=96)\n",
    "pca_1.fit(X_train_hat)\n",
    "\n",
    "pca_2 = PCA(n_components=30,random_state=96)\n",
    "pca_2.fit(X_train)\n",
    "\n",
    "pca_3 = PCA(n_components=dim_latent_optimale,random_state=96)\n",
    "pca_3.fit(latent_repre)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(np.cumsum(pca_1.explained_variance_ratio_))\n",
    "plt.plot(np.cumsum(pca_2.explained_variance_ratio_))\n",
    "plt.legend([\"X_train_hat\",\"X_train\"])\n",
    "plt.title(\"PCA - Taux informations capturé\")\n",
    "plt.xlabel(\"Nombre de dimensions\")\n",
    "plt.ylabel(\"Taux de variance capturée\")\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(np.cumsum(pca_3.explained_variance_ratio_))\n",
    "plt.title(\"PCA - Taux informations capturé sur la representation latente\")\n",
    "plt.xlabel(\"Nombre de dimensions\")\n",
    "plt.ylabel(\"Taux de variance capturée\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "quantite_info_apres_decodage = pca_1.explained_variance_ratio_[:8]\n",
    "quantite_info_espace_latent = pca_3.explained_variance_ratio_\n",
    "print(\"La quantité d'information moyenne perdue pendant le decodage : \",\n",
    "      np.abs( quantite_info_apres_decodage - quantite_info_espace_latent ).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdb01ac",
   "metadata": {},
   "source": [
    "- Le taux d'informations capturé sur la representation latente converge vers un taux de 100% au bout de 6 dimensions . \n",
    "- On remarque aussi que le taux d'informations augmente plutot rapidement au fil des dimensions , ce qui veut dire que l'encodage de l'information est plutot pas mal.\n",
    "- Conclusion : la dimension ideale d'arrivée de l'encodeur peut etre fixé à 6 .\n",
    "\n",
    "\n",
    "- Le taux d'informations recuperé sur les données issues de l'espace latent augmente plus rapidement par rapport à celles des données de l'espace d'origine.  \n",
    "- Le taux d'informations recuperé sur les données issues de l'espace latent arrivé à 100% au bout de 30 dimensions contrairement à celles des données de l'espace d'origine qui arrve uniquement à 80% de l'informations.\n",
    "- la difference de la quantité d'informations recuperés dans l'espace latent et celle du decodage de cet espace doit etre minimale pour pouvoir dire que le decodage a pu garder l'information de l'espace latent ( l'information de l'espace d'origine reduites en dimension )  \n",
    "- Un bon espace latent (regroupant le max d'informations de l'espace d'origine) permet de decoder de nouvelles images avec un pattern de chiffre parfait ( prend en compte le pattern de tous les chiffres de la classe y compris les images bruités).\n",
    "- les données USPS d'origine présentent une quantité de bruit et de la redondance, ce qui peut brouiller la structure sous-jacente des données. Dans ce cas, la compression de dimension (l'encodage) aide à éliminer une partie du bruit et de la redondance, ce qui peut conduire à une PCA de meilleure qualité pour la représentation de dimension réduite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_hat_pca = pca_1.transform(X_train_hat)\n",
    "X_train_pca = pca_2.transform(X_train)\n",
    "latent_repre_pca = pca_3.transform(latent_repre)\n",
    "\n",
    "plt.figure(figsize=(20,7))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.scatter(X_train_hat_pca[:,0],X_train_hat_pca[:,1],c=Y_train)\n",
    "plt.title(\"Nuage de points X_Train_Hat sur deux dimension\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(X_train_pca[:,0],X_train_pca[:,1],c=Y_train)\n",
    "plt.title(\"Nuage de points X_Train sur deux dimension\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(latent_repre_pca[:,0],latent_repre_pca[:,1],c=Y_train)\n",
    "plt.title(\"Nuage de points representation latent sur deux dimension\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "fig.add_subplot(projection='3d')\n",
    "\n",
    "plt.title(\"Nuage de points 3D X_Train_Hat\")\n",
    "plt.scatter(X_train_hat_pca[:,0],X_train_hat_pca[:,1],X_train_hat_pca[:,2],c=Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "fig.add_subplot(projection='3d')\n",
    "\n",
    "plt.title(\"Nuage de points 3D X_train\")\n",
    "plt.scatter(X_train_pca[:,0],X_train_pca[:,1],X_train_pca[:,2],c=Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4fdbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "fig.add_subplot(projection='3d')\n",
    "\n",
    "plt.title(\"Nuage de points 3D espace latent\")\n",
    "plt.scatter(latent_repre_pca[:,0],latent_repre_pca[:,1],latent_repre_pca[:,2],c=Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b64eac1",
   "metadata": {},
   "source": [
    "### Impact du batch sur l'apprentissage de l'espace latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = X_train.shape[1]\n",
    "nb_layers = nb_layer_optimal\n",
    "output_dim = dim_latent_optimale\n",
    "\n",
    "size = len(auto_encodeur_optimal_rand.layers_encodeur)\n",
    "auto_encodeur_optimal_rand.layers_encodeur = np.array(auto_encodeur_optimal_rand.layers_encodeur)\n",
    "indices = [i for i in range(size) if i % 2 == 1 ]\n",
    "activ_enc = auto_encodeur_optimal_rand.layers_encodeur[indices]\n",
    "\n",
    "auto_encodeur_optimal_rand.layers_decodeur = np.array(auto_encodeur_optimal_rand.layers_decodeur)\n",
    "indices = [i for i in range(size) if i % 2 == 1 ]\n",
    "activ_dec = list(auto_encodeur_optimal_rand.layers_decodeur[indices])\n",
    "\n",
    "activations = [ activ_enc , activ_dec]\n",
    "\n",
    "facteur_norma = 0.8\n",
    "plage_biais = (0,1)\n",
    "epochs = 50\n",
    "liste_batch_size = [50,300,500]\n",
    "liste_auto_encodeurs = []\n",
    "\n",
    "for batch_size in liste_batch_size:\n",
    "\n",
    "    liste_auto_encodeurs.append(create_auto_encodeur(X_train,output_dim, nb_layers,activations, \n",
    "                                                 facteur_norma, plage_biais, batch_size, epochs,eps=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be630ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, init='pca',n_iter=500, verbose=0)\n",
    "\n",
    "for i,auto_encodeur in enumerate(liste_auto_encodeurs):\n",
    "    \n",
    "    latent_repre = auto_encodeur.encode(X_train)\n",
    "    X_train_hat = auto_encodeur.decode(latent_repre)\n",
    "    \n",
    "    X_train_hat_pca = pca_1.transform(X_train_hat)\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    repre_2D = tsne.fit_transform(X_train_hat)\n",
    "    plt.title(\"données USPS train induites de l'espace latent\")\n",
    "    plt.scatter(repre_2D[:,0],repre_2D[:,1],c=Y_train)\n",
    "    plt.legend([\"Batch_size : \"+str(liste_batch_size[i])])\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title(\"Image reconstruit\")\n",
    "    image = X_train_hat[Y_train == 0][27].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "    plt.legend([\"Batch_size : \"+str(liste_batch_size[i])])\n",
    "    \n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title(\"Nuage de points X_Train_Hat\")\n",
    "    plt.scatter(X_train_hat_pca[:,0],X_train_hat_pca[:,1],c=Y_train)\n",
    "    plt.legend([\"Batch_size : \"+str(liste_batch_size[i])])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc1ee48",
   "metadata": {},
   "source": [
    "## Debruitage de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47deeb15",
   "metadata": {},
   "source": [
    "#### Bruit poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f720ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamdas = [1e-3,1e-1,1,3]\n",
    "scores = []\n",
    "\n",
    "for i,lamda in enumerate(lamdas):\n",
    "    noise = generate_noise(X_train.shape[1],'poisson',lamda,1e-1)\n",
    "    X_train_noised = X_train + noise\n",
    "    \n",
    "    latent_repre_noised = auto_encodeur_optimal_rand.encode(X_train_noised)\n",
    "    X_train_noised_hat = auto_encodeur_optimal_rand.decode(latent_repre_noised)\n",
    "    sample = 1\n",
    "    plt.figure(figsize=(15,20))\n",
    "\n",
    "    plt.subplot(i+1,3,1)\n",
    "    plt.title(\"Original \")\n",
    "    image = X_train[sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "\n",
    "\n",
    "    plt.subplot(i+1,3,2)\n",
    "    plt.title(\"Original noised - poisson(\"+str(lamda)+\")\")\n",
    "    image = X_train_noised[sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "\n",
    "\n",
    "    plt.subplot(i+1,3,3)\n",
    "    plt.title(\"Reconstruite - poisson(\"+str(lamda)+\")\")\n",
    "    image = X_train_noised_hat[sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    datas = [X_train_noised,latent_repre_noised,X_train_noised_hat]\n",
    "    scores.append(evaluation(datas,Y_train,verbose=False)[0][2])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4277a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "plt.title(\"Score sur les données en fonction du lambda\")\n",
    "plt.xlabel(\"Lambda - Bruit poisson\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Train\"])\n",
    "plt.plot(lamdas,scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Bruit gaussien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4074ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [1e-3,1e-1,0.5,3]\n",
    "stds = [1e-3,1e-1,0.5,3]\n",
    "scores = []\n",
    "\n",
    "\n",
    "for i,mean in enumerate(means):\n",
    "    noise = generate_noise(X_train.shape[1],'gaussien',mean,stds[i])\n",
    "    X_train_noised = X_train + noise\n",
    "    \n",
    "    latent_repre_noised = auto_encodeur_optimal_rand.encode(X_train_noised)\n",
    "    X_train_noised_hat = auto_encodeur_optimal_rand.decode(latent_repre_noised)\n",
    "    sample = 1\n",
    "    plt.figure(figsize=(15,20))\n",
    "\n",
    "    plt.subplot(i+1,3,1)\n",
    "    plt.title(\"Original \")\n",
    "    image = X_train[sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "\n",
    "\n",
    "    plt.subplot(i+1,3,2)\n",
    "    plt.title(\"Original noised - normal(\"+str(mean)+\",\"+str(stds[i])+\")\")\n",
    "    image = X_train_noised[sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "\n",
    "\n",
    "    plt.subplot(i+1,3,3)\n",
    "    plt.title(\"Reconstruite - normal(\"+str(mean)+\",\"+str(stds[i])+\")\")\n",
    "    image = X_train_noised_hat[sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    datas = [X_train_noised,latent_repre_noised,X_train_noised_hat]\n",
    "    scores.append(evaluation(datas,Y_train,verbose=False)[0][2])\n",
    "    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "plt.title(\"Score sur les données en fonction de la moyenne\")\n",
    "plt.xlabel(\"Mean - Bruit gaussien\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Train\"])\n",
    "plt.plot(means,scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa540e5",
   "metadata": {},
   "source": [
    "- Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = generate_noise(X_train.shape[1],'poisson',1e-1,1e-1)\n",
    "X_train_noised = X_train + noise\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Image USPS originale\")\n",
    "plt.imshow(X_train[10].reshape((16,16)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Image USPS bruité\")\n",
    "plt.imshow(X_train_noised[10].reshape((16,16)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd0fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "sample = np.random.randint(0,50)\n",
    "\n",
    "latent_repre_noised = auto_encodeur_optimal_rand.encode(X_train_noised)\n",
    "X_train_noised_hat = auto_encodeur_optimal_rand.decode(latent_repre_noised)\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" original\")\n",
    "    image = X_train[Y_train == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" original noised\")\n",
    "    image = X_train_noised[Y_train == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" reconstruit\")\n",
    "    image = X_train_noised_hat[Y_train == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [X_train_noised,latent_repre_noised,X_train_noised_hat]\n",
    "evaluation(datas,Y_train,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc5baa4",
   "metadata": {},
   "source": [
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = generate_noise(X_test.shape[1],'poisson',1e-2,1e-1)\n",
    "X_test_noised = X_test + noise\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Image USPS originale\")\n",
    "plt.imshow(X_test[10].reshape((16,16)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Image USPS bruité\")\n",
    "plt.imshow(X_test_noised[10].reshape((16,16)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_repre_noised = auto_encodeur_optimal_rand.encode(X_test_noised)\n",
    "X_test_noised_hat = auto_encodeur_optimal_rand.decode(latent_repre_noised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee8af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "sample = np.random.randint(0,50)\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" original\")\n",
    "    image = X_test[Y_test == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" original noised\")\n",
    "    image = X_test_noised[Y_test == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.title(str(i)+\" reconstruit\")\n",
    "    image = X_test_noised_hat[Y_test == i][sample].reshape((16,16))\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [X_test_noised,latent_repre_noised,X_test_noised_hat]\n",
    "evaluation(datas,Y_test,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f5b624",
   "metadata": {},
   "source": [
    "# 2. Données Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "data = load_wine()\n",
    "labels = data.target\n",
    "\n",
    "wine_data_norm = normalisation(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d82f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 3\n",
    "nb_layers = 2\n",
    "TanH = ModuleTanH()\n",
    "sig = ModuleSigmoide()\n",
    "activ_enc = [TanH for _ in range(2)]\n",
    "activ_dec = [TanH for _ in range(1)]\n",
    "activ_dec.append(sig)\n",
    "\n",
    "activations = [activ_enc , activ_dec]\n",
    "\n",
    "facteur_norma = 0.8\n",
    "plage_biais = (0,1)\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "wine_auto_encodeur = create_auto_encodeur(wine_data_norm,output_dim, nb_layers,activations, \n",
    "                                                 facteur_norma, plage_biais, batch_size, epochs,eps=1e-4)\n",
    "\n",
    "\n",
    "net_to_graph(wine_auto_encodeur.network, net_name=\"networks_images/network_AE_wine\", horizontal=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48208ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,7))\n",
    "\n",
    "pca = PCA(n_components=2,random_state=96)\n",
    "pca.fit(wine_data_norm)\n",
    "wine_data_norm_pca = pca.transform(wine_data_norm)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"PCA sur l'espace d'origine\")\n",
    "plt.scatter(wine_data_norm_pca[:,0],wine_data_norm_pca[:,1],c=labels)\n",
    "\n",
    "latent_repre = wine_auto_encodeur.encode(wine_data_norm)\n",
    "plt.subplot(122)\n",
    "plt.title(\"Espace latent\")\n",
    "plt.scatter(latent_repre[:,0],latent_repre[:,1],c=labels)\n",
    "\n",
    "\n",
    "latent_repre = wine_auto_encodeur.encode(wine_data_norm)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(wine_data_norm,labels)\n",
    "print(\"Accuracy sur l'espace d'origine : \",clf.score(wine_data_norm,labels))\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, max_iter=100).fit(wine_data_norm)\n",
    "yhat = kmeans.labels_\n",
    "\n",
    "print(\"Puerete du clustering sur l'espace d'origine: \",cluster_purity(yhat,labels))\n",
    "print()\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(latent_repre,labels)\n",
    "print(\"Accuracy sur l'espace latent : \",clf.score(latent_repre,labels))\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, max_iter=100).fit(latent_repre)\n",
    "yhat = kmeans.labels_\n",
    "\n",
    "print(\"Puerete du clustering sur l'espace latent : \",cluster_purity(yhat,labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb035df9",
   "metadata": {},
   "source": [
    "# 3. Données Crédit Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec939e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data = pd.read_csv(\"data/CC GENERAL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80547b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86362e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprimer les valeurs NaN\n",
    "cc_data = cc_data.dropna()\n",
    "\n",
    "cc_data = np.array(cc_data)\n",
    "# supprimer la colonne 1\n",
    "cc_data = cc_data[:,1:]\n",
    "cc_data = cc_data.astype('float')\n",
    "y = np.random.randint(0,1,cc_data.shape[0])\n",
    "\n",
    "data_norm = normalisation(cc_data)\n",
    "\n",
    "data_norm = data_norm.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = data_norm.shape[1]\n",
    "nb_layers = 2\n",
    "output_dim = 7\n",
    "\n",
    "TanH = ModuleTanH()\n",
    "sig = ModuleSigmoide()\n",
    "activ_enc = [TanH for _ in range(nb_layers)]\n",
    "activ_dec = [TanH for _ in range(nb_layers - 1)]\n",
    "activ_dec.append(sig)\n",
    "\n",
    "activations = [activ_enc , activ_dec]\n",
    "\n",
    "\n",
    "facteur_norma = 0.8\n",
    "plage_biais = (0,1)\n",
    "batch_size = 50\n",
    "epochs = 50\n",
    "\n",
    "auto_encodeur_cc_optimal= create_auto_encodeur(data_norm,output_dim, nb_layers,activations, \n",
    "                                                 facteur_norma, plage_biais, batch_size, epochs,eps=1e-4)\n",
    "\n",
    "net_to_graph(auto_encodeur_cc_optimal.network, net_name=\"networks_images/network_AE_credit_card\", horizontal=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd8901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_repre = auto_encodeur_cc_optimal.encode(data_norm)\n",
    "data_norm_hat = auto_encodeur_cc_optimal.decode(latent_repre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b328267",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=0, max_iter=100).fit(data_norm_hat)\n",
    "\n",
    "plt.figure(figsize=(20,7))\n",
    "\n",
    "yhat = kmeans.labels_\n",
    "\n",
    "plt.subplot(121)\n",
    "tsne = TSNE(n_components=2, init='pca',n_iter=500, verbose=0)\n",
    "repre_2D = tsne.fit_transform(data_norm_hat)\n",
    "\n",
    "plt.title(\"Representation des données induites de l'espace latent apres une t-SNE sur CCData\")\n",
    "plt.scatter(repre_2D[:,0],repre_2D[:,1],c=yhat)\n",
    "\n",
    "plt.subplot(122)\n",
    "tsne = TSNE(n_components=2, init='pca',n_iter=500, verbose=0)\n",
    "repre_2D = tsne.fit_transform(latent_repre)\n",
    "\n",
    "plt.title(\"Representation de l'espace latent apres une t-SNE sur CCData\")\n",
    "plt.scatter(repre_2D[:,0],repre_2D[:,1],c=yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=17,random_state=96)\n",
    "pca.fit(data_norm)\n",
    "X_train_pca = pca.transform(data_norm)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,7))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X_train_pca[:,0],X_train_pca[:,1],c=yhat)\n",
    "plt.title(\"PCA sur les données originale CCdata\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"PCA sur l'espace latent des données CCdata\")\n",
    "plt.scatter(latent_repre[:,0],latent_repre[:,1],c=yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
